{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import subprocess\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "from datetime import timedelta\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import clear_output\n",
    "from joblib import dump, load\n",
    "\n",
    "# Preprocessing and EDA\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, roc_curve, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## Feature-scaling stack\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "## Dimesionality reduction\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "# Deep learning modeling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow import keras\n",
    "#from tensorflow_addons.utils.types import TensorLike\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, PrecisionAtRecall, SpecificityAtSensitivity\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "## Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Machine learning modeling\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_curve, roc_auc_score, auc, fbeta_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#set_global_policy('mixed_float16')\n",
    "set_global_policy('float32')\n",
    "#set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check for available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detected: {gpus}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### **Note: GPU execution is recommended for this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained models can be downloaded from this link: https://drive.google.com/drive/folders/1WUwwZfYQLRqRyH0z22ZfiXWt3Nzj37zd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file /kaggle/input/imputers/other/default/1/imputer_age.joblib was not found.\n",
      "The file /kaggle/input/imputers/other/default/1/imputer_sex.joblib was not found.\n",
      "The file /kaggle/input/imputers/other/default/1/imputer_numericals.joblib was not found.\n",
      "The file /kaggle/input/imputers/other/default/1/imputer_categoricals.joblib was not found.\n",
      "The file /kaggle/input/train-cnn-crossval-preds-rn152v2-v2/train-cnn-crossval-preds-rn152v2_v2.csv was not found.\n",
      "The file /kaggle/input/resnet152v2/keras/default/1/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5 was not found.\n",
      "The file /kaggle/input/resnet152v2-kaggle-1-default-v4/keras/default/1/rn152v2_nn64_lr0001_relu_batch64_epoch50_kaggle_weights_2.h5 was not found.\n",
      "The file /kaggle/input/soft-voting-classifier-v2/scikitlearn/default/1/soft_voting_classif_xgb_lgb_gbm.joblib was not found.\n"
     ]
    }
   ],
   "source": [
    "# Check out files\n",
    "imputer_age_model =         '/kaggle/input/imputers/other/default/1/imputer_age.joblib'\n",
    "imputer_sex_model =         '/kaggle/input/imputers/other/default/1/imputer_sex.joblib'\n",
    "imputer_numerical_model =   '/kaggle/input/imputers/other/default/1/imputer_numericals.joblib'\n",
    "imputer_categorical_model = '/kaggle/input/imputers/other/default/1/imputer_categoricals.joblib'\n",
    "crossval_preds =            '/kaggle/input/train-cnn-crossval-preds-rn152v2-v2/train-cnn-crossval-preds-rn152v2_v2.csv'\n",
    "#crossval_preds =            '/kaggle/input/train-cnn-crossval-preds-rn152v2-v4/train-cnn-crossval-preds-rn152v2_v4.csv'\n",
    "rn152v2_weights =           '/kaggle/input/resnet152v2/keras/default/1/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "cnn_weights =               '/kaggle/input/resnet152v2-kaggle-1-default-v4/keras/default/1/rn152v2_nn64_lr0001_relu_batch64_epoch50_kaggle_weights_2.h5'\n",
    "ml_model =                  '/kaggle/input/soft-voting-classifier-v2/scikitlearn/default/1/soft_voting_classif_xgb_lgb_gbm.joblib'\n",
    "\n",
    "try:\n",
    "    with open(imputer_age_model, 'r') as file:\n",
    "        print(f\"The file {imputer_age_model} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {imputer_age_model} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(imputer_sex_model, 'r') as file:\n",
    "        print(f\"The file {imputer_sex_model} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {imputer_sex_model} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(imputer_numerical_model, 'r') as file:\n",
    "        print(f\"The file {imputer_numerical_model} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {imputer_numerical_model} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(imputer_categorical_model, 'r') as file:\n",
    "        print(f\"The file {imputer_categorical_model} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {imputer_categorical_model} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "    \n",
    "try:\n",
    "    with open(crossval_preds, 'r') as file:\n",
    "        print(f\"The file {crossval_preds} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {crossval_preds} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(rn152v2_weights, 'r') as file:\n",
    "        print(f\"The file {rn152v2_weights} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {rn152v2_weights} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "    \n",
    "try:\n",
    "    with open(cnn_weights, 'r') as file:\n",
    "        print(f\"The file {cnn_weights} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {cnn_weights} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")\n",
    "    \n",
    "try:\n",
    "    with open(ml_model, 'r') as file:\n",
    "        print(f\"The file {ml_model} is accessible.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {ml_model} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while trying to access the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE = {\n",
    "    'eda':         1,\n",
    "    'fe-dt':       1,\n",
    "    'train-cnn':   0,\n",
    "    'load-cnn-bk': 1,\n",
    "    'train-ml':    1,\n",
    "    'inference':   1,\n",
    "    'profiling':   1\n",
    "}\n",
    "\n",
    "RUN_ON_KAGGLE = 1\n",
    "\n",
    "if RUN_ON_KAGGLE == 1:\n",
    "    ROOT_DATASET_DIR = \"/kaggle/input\"\n",
    "else:\n",
    "    ROOT_DATASET_DIR = \"../\"\n",
    "\n",
    "# Convert from seconds to hhmmss\n",
    "def hhmmss(seconds):\n",
    "    td = timedelta(seconds=seconds)\n",
    "    return str(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['profiling'] == 1:\n",
    "    start_total = time.time()\n",
    "    start = start_total\n",
    "\n",
    "if ENABLE['eda'] == 1:\n",
    "    # Read the dataset\n",
    "    IMAGE_PATH = os.path.join(ROOT_DATASET_DIR,\"isic-2024-challenge\",\"train-image\",\"image\")\n",
    "    file_name_train = os.path.join(ROOT_DATASET_DIR,\"isic-2024-challenge\",\"train-metadata.csv\")\n",
    "    df_raw_train = pd.read_csv(file_name_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply head\n",
    "if ENABLE['eda'] == 1:\n",
    "    display(df_raw_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appli describe\n",
    "if ENABLE['eda'] == 1:\n",
    "    stats = df_raw_train.describe().T.head()\n",
    "    display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the number of samples in the dataframe\n",
    "if ENABLE['eda'] == 1:\n",
    "    print(f\"Number of samples in the training dataframe: {df_raw_train.shape[0]}\")\n",
    "    print(f\"Number of features in the training dataframe: {df_raw_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the data types\n",
    "if ENABLE['eda'] == 1:\n",
    "    display(df_raw_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['eda'] == 1:\n",
    "\n",
    "    targets = df_raw_train.target.value_counts()\n",
    "    print(targets)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    plt.title('Target')\n",
    "    sns.countplot(data=df_raw_train, x='target', hue='target', palette=['green', 'red'], ax=ax)\n",
    "    ax.legend_.remove()\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Benign', 'Malign'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is extremely imbalanced. There are much more benign cases than malign cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['eda'] == 1:\n",
    "    # Features that are not included in the test dataset\n",
    "    columns_to_be_dropped_train = ['lesion_id','iddx_full','iddx_1','iddx_2','iddx_3','iddx_4','iddx_5','mel_mitotic_index','mel_thick_mm','tbp_lv_dnn_lesion_confidence','patient_id','image_type','attribution','copyright_license']\n",
    "    df_dropped_train = df_raw_train.drop(columns=columns_to_be_dropped_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print NaN values only\n",
    "def print_NaNs(df):\n",
    "    nan_list = df.isna().sum()\n",
    "    if nan_list.sum() == 0:\n",
    "        print(\"The dataframe contains no NaN values\")\n",
    "    else:\n",
    "        return nan_list[nan_list != 0]\n",
    "\n",
    "if ENABLE['eda'] == 1:\n",
    "    print('Train: ', end='', flush=True)\n",
    "    display(print_NaNs(df_dropped_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NaN Values\n",
    "\n",
    "Given the high number of benign cases, it would be possible simple to remove those raws with at least one NaN value. Howerve, it would also remove some malign cases. Due to the very low number of malign cases, it would be preferable to keep them. In such a case, we can replace the NaN values of ages with the median, thos of sex with the mode, and those of anatom_site_general can be removed as it is very difficult to estimate the region of the cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below code we will generate two imputing models to complete missing information in the \"age_approx\" and \"sex\" fields.\n",
    "if ENABLE['eda'] == 1:\n",
    "    \n",
    "    # Replace and remove NaN values\n",
    "    IMPUTER_AGE = KNNImputer(n_neighbors=5)\n",
    "    age_approx_reshaped_train = df_dropped_train[['age_approx']]\n",
    "    df_dropped_train['age_approx'] = IMPUTER_AGE.fit_transform(age_approx_reshaped_train)\n",
    "    IMPUTER_SEX = KNNImputer(n_neighbors=5)\n",
    "    df_dropped_train['sex'] = df_dropped_train['sex'].apply(lambda x: 0 if x == 'male' else 1 if x == 'female' else np.NaN)\n",
    "    sex_reshaped_train = df_dropped_train[['sex']]\n",
    "    df_dropped_train['sex'] = IMPUTER_SEX.fit_transform(sex_reshaped_train)\n",
    "    df_dropped_train.dropna(subset=['anatom_site_general'], inplace=True)\n",
    "    print_NaNs(df_dropped_train)\n",
    "\n",
    "    # Determine how many cases have been removed\n",
    "    print(f\"Number of samples before removing NaNs: {df_raw_train.shape[0]}\")\n",
    "    print(f\"Number of samples after removing NaNs:  {df_dropped_train.shape[0]}\")\n",
    "    print(f\"Data reduccion in percentage: {np.round(100 * (df_dropped_train.shape[0] - df_raw_train.shape[0]) / df_raw_train.shape[0], 1)}%\")\n",
    "\n",
    "    targets_dropped = df_dropped_train.target.value_counts()\n",
    "    print(f\"Targets before dropping:\")\n",
    "    print(targets)\n",
    "    print(f\"Targets after dropping:\")\n",
    "    print(targets_dropped)\n",
    "    \n",
    "    # Save imputers\n",
    "    #imputer_age_model = 'imputer_age.joblib'\n",
    "    #imputer_sex_model = 'imputer_sex.joblib'\n",
    "    #dump(IMPUTER_AGE, imputer_age_model)\n",
    "    #dump(IMPUTER_SEX, imputer_sex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['eda'] == 1:\n",
    "    \n",
    "    # Convert target, sex, and tile_type into boolean (or int with 0 and 1) and others to categorical\n",
    "\n",
    "    # target\n",
    "    df_dropped_train['target'] = df_dropped_train['target'].astype(int)\n",
    "\n",
    "    # tdb_tile_type -> 0: white, 1: XP\n",
    "    df_dropped_train['tbp_tile_type'] = df_dropped_train['tbp_tile_type'].apply(lambda x: 0 if x == '3D: white' else 1)\n",
    "    df_dropped_train['tbp_tile_type'] = df_dropped_train['tbp_tile_type'].astype(int)\n",
    "\n",
    "    # Convert anatom_site_general, tbp_lv_location, tbp_lv_location_simple, and sex into categorical\n",
    "    df_dropped_train['anatom_site_general'] = pd.Categorical(df_dropped_train['anatom_site_general'])\n",
    "    df_dropped_train['tbp_lv_location'] = pd.Categorical(df_dropped_train['tbp_lv_location'])\n",
    "    df_dropped_train['tbp_lv_location_simple'] = pd.Categorical(df_dropped_train['tbp_lv_location_simple'])\n",
    "    df_dropped_train['sex'] = pd.Categorical(df_dropped_train['sex'])\n",
    "    print(df_dropped_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution Analysis: Skewness Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['eda'] == 1:\n",
    "    \n",
    "    num_features = list(df_dropped_train.columns[(df_dropped_train.dtypes != object) & (df_dropped_train.dtypes != 'category')])\n",
    "    num_features.remove('target')\n",
    "    def plot_num(df, feature, label=''):\n",
    "        feature = label + feature\n",
    "        fig = plt.figure(figsize=(20,1))\n",
    "        axes = fig.add_subplot(121)    \n",
    "        sns.boxplot(data=df, x=feature, ax=axes)    \n",
    "        axes = fig.add_subplot(122)\n",
    "        sns.histplot(data=df, x=feature, ax=axes, color='#D0312D', kde=True)\n",
    "        fig.set_size_inches(20, 1)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_num_comp(df1, df2, feature, label1='', label2=''):\n",
    "        feature1 = label1 + feature\n",
    "        feature2 = label2 + feature\n",
    "        fig = plt.figure(figsize=(20,2))\n",
    "        axes = fig.add_subplot(141)    \n",
    "        sns.boxplot(data=df1, x=feature1, ax=axes)    \n",
    "        axes = fig.add_subplot(142)\n",
    "        sns.histplot(data=df1, x=feature1, ax=axes, color='#D0312D', kde=True)\n",
    "        axes = fig.add_subplot(143)    \n",
    "        sns.boxplot(data=df2, x=feature2, ax=axes)    \n",
    "        axes = fig.add_subplot(144)\n",
    "        sns.histplot(data=df2, x=feature2, ax=axes, color='green', kde=True)\n",
    "        fig.set_size_inches(20, 2)\n",
    "        plt.show()\n",
    "\n",
    "    for idx, column in enumerate(num_features):\n",
    "        plot_num(df_dropped_train, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data distributions, such as \"tbp_lv_eccentricity\" and tbp_lv_perimeterMM, are right and left skewed. Transformation may be needed. Other features do not need to be transformed, for example, coordinates, or even features with negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['eda'] == 1:\n",
    "    \n",
    "    # This function anlyzes, for each feature, different transforms for correcting skeweness and returns\n",
    "    # a dataframe with the results. Ideally a data distribution should have a skewness score close to 0.\n",
    "    # Good skeness score are around +- 0.5\n",
    "    def analyze_skewness(df):\n",
    "\n",
    "        # Obtain the list of the column names\n",
    "        num_features = list(df.columns[(df.dtypes != object) & (df.dtypes != 'category')])\n",
    "        if 'target' in df:\n",
    "            num_features.remove('target')\n",
    "\n",
    "        # Define the number of rows and columns\n",
    "        num_rows = len(num_features)\n",
    "        columns = ['Feature', 'ORIG', 'LOG', 'SQR', 'RSQR']\n",
    "\n",
    "        # Initialize the dataframe with zeros\n",
    "        df_stats = pd.DataFrame(np.zeros((num_rows, len(columns))), columns=columns)\n",
    "\n",
    "        # Loop over features\n",
    "        for idx, feature in enumerate(num_features):\n",
    "\n",
    "            tr_log = ColumnTransformer(transformers=[(\"log\", FunctionTransformer(np.log1p), [feature])])\n",
    "            tr_sqr = ColumnTransformer(transformers=[(\"sqr\", FunctionTransformer(np.square), [feature])])    \n",
    "            tr_sqrt = ColumnTransformer(transformers=[(\"sqrt\", FunctionTransformer(np.sqrt), [feature])])\n",
    "\n",
    "            df_log = pd.DataFrame(tr_log.fit_transform(df))\n",
    "            df_sqr = pd.DataFrame(tr_sqr.fit_transform(df))\n",
    "            df_sqrt = pd.DataFrame(tr_sqrt.fit_transform(df))\n",
    "\n",
    "            df_stats.iloc[idx,0] = feature\n",
    "            df_stats.iloc[idx,1] = round(df[feature].skew(),1)\n",
    "            df_stats.iloc[idx,2] = round(df_log.skew()[0],1)\n",
    "            df_stats.iloc[idx,3] = round(df_sqr.skew()[0],1)\n",
    "            df_stats.iloc[idx,4] = round(df_sqrt.skew()[0],1)\n",
    "\n",
    "        return pd.DataFrame(df_stats).set_index('Feature')\n",
    "    \n",
    "    # Display metrics!\n",
    "    display(analyze_skewness(df_dropped_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Dataframe After EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['eda'] == 1:\n",
    "    \n",
    "    # According to the above table, some features may require log transform and square \n",
    "    # transform to correct skewness. Based on an analysis of the impact of skewennes \n",
    "    # correction on KBest (f_classif), it is concluded that only feature \"tbp_lv_eccentricity\"\n",
    "    # shall be sqr transformed to have more discriminative power.\n",
    "    feature_to_be_sqrtr = ['tbp_lv_eccentricity']\n",
    "    df_sqr_feature = df_dropped_train[feature_to_be_sqrtr].apply(lambda x : np.square(x))\n",
    "    df_sqr_feature.columns = ['sqr_tbp_lv_eccentricity']\n",
    "    \n",
    "    # Merge dataframe and drop the original feature\n",
    "    df_eda_train = pd.concat([df_dropped_train, df_sqr_feature], axis=1)\n",
    "    df_eda_train = df_eda_train.reset_index(drop=True)\n",
    "    print(f\"Columns df_dropped_train: {df_dropped_train.shape[1]}\")\n",
    "    print(f\"Columns df_eda_train: {df_eda_train.shape[1]}\")\n",
    "    \n",
    "    # Verify that the dataframe has no NaN values\n",
    "    print_NaNs(df_eda_train)\n",
    "    \n",
    "    #df_dropped.to_csv(\"train-metadata-eda.csv\")\n",
    "    #df_eda.to_csv(\"train-metadata-eda.csv\", index=False)\n",
    "\n",
    "#else:\n",
    "    \n",
    "    # Load dataframe with eda    \n",
    "    #df_eda_train = pd.read_csv(os.path.join(ROOT_DATASET_DIR,\"dataframe-eda\",\"train-metadata-eda.csv\"))\n",
    "\n",
    "if ENABLE['profiling'] == 1:\n",
    "    endt = time.time()\n",
    "    eda_t = hhmmss(endt - start)\n",
    "    print(f\"\\nEDA time: {eda_t}.\")\n",
    "    print(f\"Total time: {hhmmss(endt - start_total)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering: Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features can be created by combining the existing ones in a meaninfull way. The new features are taken from the following notebook: https://www.kaggle.com/code/vyacheslavbolotin/isic-2024-only-tabular-data-new-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['profiling'] == 1:\n",
    "    start = time.time()\n",
    "\n",
    "if ENABLE['fe-dt'] == 1:\n",
    "\n",
    "    # Function to apply feature engineering\n",
    "    def apply_fe(df):\n",
    "        \n",
    "        # Create new features\n",
    "        n_rows = df.shape[0]\n",
    "\n",
    "        new_cols = [\n",
    "            'lesion_size_ratio',                 # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n",
    "            'lesion_shape_index',                # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n",
    "            'hue_contrast',                      # tbp_lv_H                - tbp_lv_Hext              abs\n",
    "            'luminance_contrast',                # tbp_lv_L                - tbp_lv_Lext              abs\n",
    "            'lesion_color_difference',           # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n",
    "            'border_complexity',                 # tbp_lv_norm_border      + tbp_lv_symm_2axis\n",
    "\n",
    "            'position_distance_3d',              # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n",
    "            'log_perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2  np.log1p(x)\n",
    "            'area_to_perimeter_ratio',           # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n",
    "            'lesion_visibility_score',           # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n",
    "            'symmetry_border_consistency',       # tbp_lv_symm_2axis       * tbp_lv_norm_border\n",
    "            'consistency_symmetry_border',       # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n",
    "\n",
    "            'color_consistency',                 # tbp_lv_stdL             / tbp_lv_Lext\n",
    "            'consistency_color',                 # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n",
    "            'size_age_interaction',              # clin_size_long_diam_mm  * age_approx\n",
    "            'hue_color_std_interaction',         # tbp_lv_H                * tbp_lv_color_std_mean\n",
    "            'lesion_severity_index',             # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n",
    "            'shape_complexity_index',            # border_complexity       + lesion_shape_index\n",
    "            'color_contrast_index',              # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n",
    "\n",
    "            'normalized_lesion_size',            # clin_size_long_diam_mm  / age_approx\n",
    "            'mean_hue_difference',               # tbp_lv_H                + tbp_lv_Hext    / 2\n",
    "            'std_dev_contrast',                  # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n",
    "            'color_shape_composite_index',       # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n",
    "            'lesion_orientation_3d',             # tbp_lv_y                , tbp_lv_x  np.arctan2\n",
    "            'overall_color_difference',          # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n",
    "\n",
    "            'symmetry_perimeter_interaction',    # tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n",
    "            'comprehensive_lesion_index',        # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n",
    "            'color_variance_ratio',              # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n",
    "            'border_color_interaction',          # tbp_lv_norm_border      * tbp_lv_norm_color\n",
    "            'border_color_interaction_2',\n",
    "            'size_color_contrast_ratio',         # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n",
    "            'log_age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx  np.log1p(x)\n",
    "            'age_normalized_nevi_confidence_2',\n",
    "            'color_asymmetry_index',             # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n",
    "\n",
    "            'volume_approximation_3d',           # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n",
    "            'color_range',                       # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n",
    "            'shape_color_consistency',           # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n",
    "            'border_length_ratio',               # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n",
    "            'age_size_symmetry_index',           # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n",
    "            'index_age_size_symmetry',           # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n",
    "        ]\n",
    "\n",
    "        # Initialize a DataFrame with zeros\n",
    "        df_nf = pd.DataFrame(np.zeros((n_rows, len(new_cols))), columns=new_cols)\n",
    "\n",
    "        # Calculate the new features\n",
    "        df_nf['lesion_size_ratio'] = df['tbp_lv_minorAxisMM'] / (df['clin_size_long_diam_mm'] + 1e-5)\n",
    "        df_nf['lesion_shape_index'] = df['tbp_lv_areaMM2'] / ((df['tbp_lv_perimeterMM'] ** 2) + 1e-5)\n",
    "        df_nf['hue_contrast'] = (df['tbp_lv_H'] - df['tbp_lv_Hext']).abs()\n",
    "        df_nf['luminance_contrast'] = (df['tbp_lv_L'] - df['tbp_lv_Lext']).abs()\n",
    "        df_nf['lesion_color_difference'] = (df['tbp_lv_deltaA'] ** 2 + df['tbp_lv_deltaB'] ** 2 + df['tbp_lv_deltaL'] ** 2).pow(0.5)\n",
    "        df_nf['border_complexity'] = df['tbp_lv_norm_border'] + df['tbp_lv_symm_2axis']\n",
    "\n",
    "        df_nf['position_distance_3d'] = (df['tbp_lv_x'] ** 2 + df['tbp_lv_y'] ** 2 + df['tbp_lv_z'] ** 2)\n",
    "        df_nf['log_perimeter_to_area_ratio'] = np.log1p(df['tbp_lv_perimeterMM'] / (df['tbp_lv_areaMM2'] + 1e-5))\n",
    "        df_nf['area_to_perimeter_ratio'] = df['tbp_lv_areaMM2'] / (df['tbp_lv_perimeterMM'] + 1e-5)\n",
    "        df_nf['lesion_visibility_score'] = df['tbp_lv_deltaLBnorm'] + df['tbp_lv_norm_color']\n",
    "        df_nf['symmetry_border_consistency'] = df['tbp_lv_symm_2axis'] * df['tbp_lv_norm_border']\n",
    "        df_nf['consistency_symmetry_border'] = df['tbp_lv_symm_2axis'] * df['tbp_lv_norm_border'] / (df['tbp_lv_symm_2axis'] + df['tbp_lv_norm_border'] + 1e-5)\n",
    "\n",
    "        df_nf['color_consistency'] = df['tbp_lv_stdL'] / (df['tbp_lv_Lext'] + 1e-5)\n",
    "        df_nf['consistency_color'] = df['tbp_lv_stdL'] * df['tbp_lv_Lext'] / (df['tbp_lv_stdL'] + df['tbp_lv_Lext'] + 1e-5)\n",
    "        df_nf['size_age_interaction'] = df['clin_size_long_diam_mm'] * df['age_approx']\n",
    "        df_nf['hue_color_std_interaction'] = df['tbp_lv_H'] * df['tbp_lv_color_std_mean']\n",
    "        df_nf['lesion_severity_index'] = (df['tbp_lv_norm_border'] + df['tbp_lv_norm_color'] + df['tbp_lv_eccentricity']) / 3\n",
    "        df_nf['shape_complexity_index'] = df_nf['border_complexity'] + df_nf['lesion_shape_index']\n",
    "        df_nf['color_contrast_index'] = df['tbp_lv_deltaA'] + df['tbp_lv_deltaB'] + df['tbp_lv_deltaL'] + df['tbp_lv_deltaLBnorm']\n",
    "\n",
    "        df_nf['normalized_lesion_size'] = df['clin_size_long_diam_mm'] / (df['age_approx'] + 1e-5)\n",
    "        df_nf['mean_hue_difference'] = (df['tbp_lv_H'] + df['tbp_lv_Hext']) / 2\n",
    "        df_nf['std_dev_contrast'] = ((df['tbp_lv_deltaA'] ** 2 + df['tbp_lv_deltaB'] ** 2 + df['tbp_lv_deltaL'] ** 2) / 3).pow(0.5)\n",
    "        df_nf['color_shape_composite_index'] = (df['tbp_lv_color_std_mean'] + df['tbp_lv_area_perim_ratio'] + df['tbp_lv_symm_2axis']) / 3\n",
    "        df_nf['lesion_orientation_3d'] = np.arctan2(df['tbp_lv_y'], df['tbp_lv_x'])\n",
    "        df_nf['overall_color_difference'] = (df['tbp_lv_deltaA'] + df['tbp_lv_deltaB'] + df['tbp_lv_deltaL']) / 3\n",
    "\n",
    "        df_nf['symmetry_perimeter_interaction'] = df['tbp_lv_symm_2axis'] * df['tbp_lv_perimeterMM']\n",
    "        df_nf['comprehensive_lesion_index'] = (df['tbp_lv_area_perim_ratio'] + df['tbp_lv_eccentricity'] + df['tbp_lv_norm_color'] + df['tbp_lv_symm_2axis']) / 4\n",
    "        df_nf['color_variance_ratio'] = df['tbp_lv_color_std_mean'] / (df['tbp_lv_stdLExt'] + 1e-5)\n",
    "        df_nf['border_color_interaction'] = df['tbp_lv_norm_border'] * df['tbp_lv_norm_color']\n",
    "        df_nf['border_color_interaction_2'] = df['tbp_lv_norm_border'] * df['tbp_lv_norm_color'] / (df['tbp_lv_norm_border'] + df['tbp_lv_norm_color'] + 1e-5)\n",
    "        df_nf['size_color_contrast_ratio'] = df['clin_size_long_diam_mm'] / (df['tbp_lv_deltaLBnorm'] + 1e-5)\n",
    "        df_nf['log_age_normalized_nevi_confidence'] = np.log1p(df['tbp_lv_nevi_confidence'] / (df['age_approx'] + 1e-5))\n",
    "        df_nf['age_normalized_nevi_confidence_2'] = (df['clin_size_long_diam_mm']**2 + df['age_approx']**2).pow(0.5)\n",
    "        df_nf['color_asymmetry_index'] = df['tbp_lv_radial_color_std_max'] * df['tbp_lv_symm_2axis']\n",
    "\n",
    "        df_nf['volume_approximation_3d'] = df['tbp_lv_areaMM2'] * (df['tbp_lv_x']**2 + df['tbp_lv_y']**2 + df['tbp_lv_z']**2).pow(0.5)\n",
    "        df_nf['color_range'] = (df['tbp_lv_L'] - df['tbp_lv_Lext']).abs() + (df['tbp_lv_A'] - df['tbp_lv_Aext']).abs() + (df['tbp_lv_B'] - df['tbp_lv_Bext']).abs()\n",
    "        df_nf['shape_color_consistency'] = df['tbp_lv_eccentricity'] * df['tbp_lv_color_std_mean']\n",
    "        df_nf['border_length_ratio'] = df['tbp_lv_perimeterMM'] / ((2 * np.pi * (df['tbp_lv_areaMM2'] / np.pi).pow(0.5)) + 1e-5)\n",
    "        df_nf['age_size_symmetry_index'] = df['age_approx'] * df['clin_size_long_diam_mm'] * df['tbp_lv_symm_2axis']\n",
    "        df_nf['index_age_size_symmetry'] = df['age_approx'] * df['tbp_lv_areaMM2'] * df['tbp_lv_symm_2axis']\n",
    "\n",
    "        df_nf['asymmetry_ratio'] = df['tbp_lv_symm_2axis'] / (df['tbp_lv_perimeterMM'] + 1e-5)\n",
    "        df_nf['asymmetry_area_ratio'] = df['tbp_lv_symm_2axis'] / (df['tbp_lv_areaMM2'] + 1e-5)\n",
    "        df_nf['color_variation_intensity'] = df['tbp_lv_norm_color'] * df['tbp_lv_deltaLBnorm']\n",
    "        df_nf['color_contrast_ratio'] = df['tbp_lv_deltaLBnorm'] / (df['tbp_lv_L'] + 1e-5)\n",
    "        df_nf['border_density'] = df['tbp_lv_norm_border'] / (df['tbp_lv_perimeterMM'] + 1e-5)\n",
    "        df_nf['area_diameter_ratio'] = df['tbp_lv_areaMM2'] / (df['clin_size_long_diam_mm']**2 + 1e-5)        \n",
    "        df_nf['3d_position_angle_xz'] = np.arctan2(df['tbp_lv_z'], df['tbp_lv_x'])\n",
    "        df_nf['lab_chroma'] = np.sqrt(df['tbp_lv_A']**2 + df['tbp_lv_B']**2)        \n",
    "        df_nf['texture_contrast'] = df['tbp_lv_stdL'] / (df['tbp_lv_L'] + 1e-5)\n",
    "        df_nf['texture_uniformity'] = 1 / (1 + df['tbp_lv_color_std_mean'])\n",
    "        \n",
    "        return df_nf\n",
    "    \n",
    "    # Execute function!\n",
    "    df_fe_train = apply_fe(df_eda_train)\n",
    "    \n",
    "    # An remove tbp_lv_eccentricity\n",
    "    df_eda_train.drop(feature_to_be_sqrtr, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Dataframe after Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-09T11:45:47.554027Z",
     "iopub.status.busy": "2024-09-09T11:45:47.553647Z",
     "iopub.status.idle": "2024-09-09T11:46:48.585200Z",
     "shell.execute_reply": "2024-09-09T11:46:48.584175Z",
     "shell.execute_reply.started": "2024-09-09T11:45:47.553982Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENABLE['fe-dt'] == 1:\n",
    "    df_eda_fe_train = pd.concat([df_eda_train,                 # original features = [df_dropped_train, df_dropped_log_sqr_corr_train]\n",
    "                                 df_fe_train],                 # new features from the orignal ones                                  \n",
    "                                axis=1)\n",
    "    df_eda_fe_train = df_eda_fe_train.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Colums df_eda_train: {df_eda_train.shape[1]}\")\n",
    "    print(f\"Colums df_eda_train + fe: {df_eda_fe_train.shape[1]}\")\n",
    "    \n",
    "    # Create (and fit) imputers for the rest of columns\n",
    "    numericals = df_eda_fe_train.select_dtypes(include=['number']).columns.tolist()\n",
    "    categoricals = [col for col in df_eda_fe_train.columns if col not in numericals]\n",
    "    numericals.remove('target') # To be removed\n",
    "    numericals.remove('age_approx') # To be removed, already KNN imputed\n",
    "    categoricals.remove('sex') # To be removed, already KNN imputed\n",
    "    IMPUTER_NUMERICALS = SimpleImputer(strategy='median')\n",
    "    IMPUTER_NUMERICALS.fit(df_eda_fe_train[numericals])\n",
    "    IMPUTER_CATEGORICALS = SimpleImputer(strategy='most_frequent')\n",
    "    IMPUTER_CATEGORICALS.fit(df_eda_fe_train[categoricals])    \n",
    "    \n",
    "    # Verify that the dataframe has no NaN values\n",
    "    print_NaNs(df_eda_fe_train)\n",
    "    \n",
    "    # Save the dataframe after feature engineering\n",
    "    df_eda_fe_train.to_csv(\"train-metadata-eda-fe-v3-kaggle.csv\", index=False)\n",
    "    \n",
    "    # Save imputers\n",
    "    #imputer_numerical_model = 'imputer_numericals.joblib'\n",
    "    #imputer_categorical_model = 'imputer_categoricals.joblib'\n",
    "    #dump(IMPUTER_NUMERICALS, imputer_numerical_model)\n",
    "    #dump(IMPUTER_CATEGORICALS, imputer_categorical_model)\n",
    "        \n",
    "if ENABLE['profiling'] == 1:\n",
    "    endt = time.time()\n",
    "    fe_t = hhmmss(endt - start)\n",
    "    print(f\"\\nFE time: {fe_t}.\")\n",
    "    print(f\"Total time: {hhmmss(endt - start_total)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the Convolutional Neural Network: ResNet152V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:48.587144Z",
     "iopub.status.busy": "2024-09-09T11:46:48.586801Z",
     "iopub.status.idle": "2024-09-09T11:46:48.597417Z",
     "shell.execute_reply": "2024-09-09T11:46:48.596486Z",
     "shell.execute_reply.started": "2024-09-09T11:46:48.587108Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['profiling'] == 1:\n",
    "    start = time.time()\n",
    "\n",
    "# Function to get all image files belonging to image_list in a given directory\n",
    "def get_folder_files(folder_path, image_list):  \n",
    "\n",
    "    files = [os.path.join(folder_path, f\"{image}.jpg\") for image in image_list]\n",
    "    return files\n",
    "\n",
    "# Function to create and write images for each file path in given directories.\n",
    "def create_and_write_img(file_paths, file_ids, file_targets, save_dir_0, save_dir_1, desc=None):\n",
    "\n",
    "    # Iterate over each file while also displaying a progress bar using tqdm\n",
    "    for file_path, file_id, file_target, in tqdm(zip(file_paths, file_ids, file_targets), ascii=True, total=len(file_ids), desc=desc, leave=True):\n",
    "\n",
    "        # Build the new file name\n",
    "        new_name = file_id + \".jpg\"\n",
    "\n",
    "        # Build the image path\n",
    "        if file_target == 0:\n",
    "            dst_img_path = os.path.join(save_dir_0, new_name)\n",
    "        else:\n",
    "            dst_img_path = os.path.join(save_dir_1, new_name)\n",
    "\n",
    "        # Copy the file from the original location to the destination directory\n",
    "        shutil.copy(file_path, dst_img_path)\n",
    "\n",
    "    return\n",
    "\n",
    "def rsync_images(src_dirs, dest_dir):\n",
    "\n",
    "    # Execute rsync for each source directory\n",
    "    for src_dir in src_dirs:\n",
    "        command = ['rsync', '-a', f'{src_dir}/', dest_dir]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "        # Print output and errors for debugging\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "    return\n",
    "\n",
    "# Function to count files in a directory\n",
    "def count_files(target_dir):\n",
    "    return len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Images for CNN-based Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:48.599129Z",
     "iopub.status.busy": "2024-09-09T11:46:48.598759Z",
     "iopub.status.idle": "2024-09-09T11:46:48.611939Z",
     "shell.execute_reply": "2024-09-09T11:46:48.611142Z",
     "shell.execute_reply.started": "2024-09-09T11:46:48.599087Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['train-cnn'] == 1:\n",
    "    \n",
    "    # Read the dataset\n",
    "    IMAGE_PATH=os.path.join(ROOT_DATASET_DIR,\"isic-2024-challenge\",\"train-image\",\"image\")\n",
    "\n",
    "    CASE_FOLDERS = os.listdir(IMAGE_PATH)\n",
    "    ROOT_IMAGE_DIR =  os.path.join(\"images\")\n",
    "    ROOT_TRAIN_DIR_0 = os.path.join(ROOT_IMAGE_DIR, \"train\", \"0\")\n",
    "    ROOT_TRAIN_DIR_1 = os.path.join(ROOT_IMAGE_DIR, \"train\", \"1\")    \n",
    "    ROOT_ALL_DIR = os.path.join(ROOT_IMAGE_DIR,\"all\",\"10\")\n",
    "\n",
    "    # Create directories if not already present\n",
    "    os.makedirs(ROOT_TRAIN_DIR_0, exist_ok=True)\n",
    "    os.makedirs(ROOT_TRAIN_DIR_1, exist_ok=True)    \n",
    "    os.makedirs(ROOT_ALL_DIR, exist_ok=True)\n",
    "        \n",
    "    # Train-test split\n",
    "    X_train = df_eda_fe_train.drop(['target'], axis=1)\n",
    "    y_train = df_eda_fe_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:48.614316Z",
     "iopub.status.busy": "2024-09-09T11:46:48.613931Z",
     "iopub.status.idle": "2024-09-09T11:46:48.625941Z",
     "shell.execute_reply": "2024-09-09T11:46:48.625114Z",
     "shell.execute_reply.started": "2024-09-09T11:46:48.614263Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['train-cnn'] == 1:\n",
    "    \n",
    "    # Copy benign cases to folder \"0\" and malign cases to folder \"1\"\n",
    "    ids_train = X_train.isic_id.to_list()\n",
    "    files_train = get_folder_files(folder_path=IMAGE_PATH, image_list=ids_train)\n",
    "\n",
    "    create_and_write_img(file_paths=files_train,\n",
    "                         file_ids=ids_train,\n",
    "                         file_targets=y_train,                     \n",
    "                         save_dir_0=ROOT_TRAIN_DIR_0,\n",
    "                         save_dir_1=ROOT_TRAIN_DIR_1,\n",
    "                         desc=f\"Train :: {IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:48.627253Z",
     "iopub.status.busy": "2024-09-09T11:46:48.626935Z",
     "iopub.status.idle": "2024-09-09T11:46:48.636052Z",
     "shell.execute_reply": "2024-09-09T11:46:48.635145Z",
     "shell.execute_reply.started": "2024-09-09T11:46:48.627221Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['train-cnn'] == 1:\n",
    "    # Copy all training images into a single directory for inference\n",
    "    src_dirs = [ROOT_TRAIN_DIR_0, ROOT_TRAIN_DIR_1]\n",
    "    dest_dir = ROOT_ALL_DIR\n",
    "    rsync_images(src_dirs, dest_dir)\n",
    "\n",
    "    # And check out that the copies to directores are successfull \n",
    "    all_dirs = src_dirs.copy()\n",
    "    all_dirs.append(dest_dir)\n",
    "    total = 0\n",
    "    subtotal = 0\n",
    "    for directory in all_dirs:\n",
    "        file_count = count_files(directory)\n",
    "        print(f\"{directory} contains {file_count} files\")\n",
    "        if directory in src_dirs:\n",
    "            subtotal = subtotal + file_count\n",
    "        total = total + file_count\n",
    "    print(f\"Total files in the train directory: {subtotal}\")\n",
    "    print(f\"Total files in {IMAGE_PATH}: {total}\")\n",
    "    print(f\"Total cases in the dataframe: {df_eda_fe_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:48.637430Z",
     "iopub.status.busy": "2024-09-09T11:46:48.637142Z",
     "iopub.status.idle": "2024-09-09T11:46:48.649380Z",
     "shell.execute_reply": "2024-09-09T11:46:48.648563Z",
     "shell.execute_reply.started": "2024-09-09T11:46:48.637398Z"
    }
   },
   "outputs": [],
   "source": [
    "# ploting model loss during training, created by Daniel:\n",
    "# https://medium.com/geekculture/how-to-plot-model-loss-while-training-in-tensorflow-9fa1a1875a5\n",
    "class plot_learning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "\n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "\n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "\n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:48.650760Z",
     "iopub.status.busy": "2024-09-09T11:46:48.650500Z",
     "iopub.status.idle": "2024-09-09T11:46:49.206876Z",
     "shell.execute_reply": "2024-09-09T11:46:49.206097Z",
     "shell.execute_reply.started": "2024-09-09T11:46:48.650728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for custom normalization\n",
    "def custom_normalization(image):\n",
    "    image = image / 255.0\n",
    "    mean = tf.constant(MEAN, dtype=image.dtype)\n",
    "    std = tf.constant(STD, dtype=image.dtype)\n",
    "    image = (image - mean) / std  # Normalize each channel\n",
    "    return image\n",
    "\n",
    "# Random rotation\n",
    "rotate_image = RandomRotation(factor=0.2)\n",
    "\n",
    "# Image augmentation\n",
    "def augment_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)  # Horizontal flip\n",
    "    image = tf.image.random_flip_up_down(image) # Vertical flip        \n",
    "    image = rotate_image(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)\n",
    "    image = tf.image.random_saturation(image, lower=0.7, upper=1.3)    \n",
    "    return image, label\n",
    "\n",
    "# Parse and process images\n",
    "def parse_image(filename, label):\n",
    "    filename = tf.squeeze(filename)\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IM_SIZE, IM_SIZE]) #, method=tf.image.ResizeMethod.LANCZOS3)\n",
    "    image = custom_normalization(image)\n",
    "    return image, label\n",
    "\n",
    "# Generate dataset from file paths\n",
    "def generate_dataset(file_paths, labels, batch_size, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# List image paths and labels\n",
    "def get_image_paths_and_labels(directory, label):\n",
    "    data_dir = Path(directory)    \n",
    "    all_image_paths = list(data_dir.glob('*/*.jpg'))\n",
    "    all_image_paths = [str(path) for path in all_image_paths]    \n",
    "    all_labels = [0 if label in str(path) else 1 for path in all_image_paths]\n",
    "    return all_image_paths, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:49.208368Z",
     "iopub.status.busy": "2024-09-09T11:46:49.208064Z",
     "iopub.status.idle": "2024-09-09T11:46:49.217506Z",
     "shell.execute_reply": "2024-09-09T11:46:49.216664Z",
     "shell.execute_reply.started": "2024-09-09T11:46:49.208335Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['train-cnn'] == 1:\n",
    "    \n",
    "    # Function to undersample the majority class and oversample the minority class\n",
    "    def balance_classes(image_paths, labels, majority_size=None, minority_size=None):\n",
    "        # Convert to numpy arrays for easier manipulation\n",
    "        image_paths = np.array(image_paths)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Separate the majority and minority classes\n",
    "        majority_class = image_paths[labels == 0]\n",
    "        majority_labels = labels[labels == 0]\n",
    "\n",
    "        minority_class = image_paths[labels == 1]\n",
    "        minority_labels = labels[labels == 1]\n",
    "\n",
    "        # Undersample the majority class if majority_size is specified\n",
    "        if majority_size and (majority_size < len(majority_class)):\n",
    "            majority_class_downsampled, majority_labels_downsampled = resample(\n",
    "                majority_class,\n",
    "                majority_labels,\n",
    "                replace=False,  # Sample without replacement\n",
    "                n_samples=majority_size,  # Number of samples after undersampling\n",
    "                random_state=42  # For reproducibility\n",
    "            )\n",
    "        else:\n",
    "            majority_class_downsampled, majority_labels_downsampled = majority_class, majority_labels\n",
    "\n",
    "        # Oversample the minority class if minority_size is specified\n",
    "        if minority_size and (minority_size > len(minority_class)):\n",
    "            minority_class_upsampled, minority_labels_upsampled = resample(\n",
    "                minority_class,\n",
    "                minority_labels,\n",
    "                replace=True,  # Sample with replacement\n",
    "                n_samples=minority_size,  # Number of samples after oversampling\n",
    "                random_state=42  # For reproducibility\n",
    "            )\n",
    "        else:\n",
    "            minority_class_upsampled, minority_labels_upsampled = minority_class, minority_labels\n",
    "\n",
    "        # Combine the undersampled majority class and upsampled minority class\n",
    "        balanced_image_paths = np.concatenate([majority_class_downsampled, minority_class_upsampled])\n",
    "        balanced_labels = np.concatenate([majority_labels_downsampled, minority_labels_upsampled])\n",
    "\n",
    "        return balanced_image_paths.tolist(), balanced_labels.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:49.218965Z",
     "iopub.status.busy": "2024-09-09T11:46:49.218623Z",
     "iopub.status.idle": "2024-09-09T11:46:49.230295Z",
     "shell.execute_reply": "2024-09-09T11:46:49.229464Z",
     "shell.execute_reply.started": "2024-09-09T11:46:49.218933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters related to the CNN architecture and training process\n",
    "BATCH_SIZE = 64\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "IM_SIZE = 128 # assumed 128x128. Some images to be scaled.\n",
    "TRAIN_TEST_SPLIT = 0.15\n",
    "EPOCHS = 50\n",
    "LEARN_RATE = 0.0001\n",
    "REG_RATE = 0.001\n",
    "NEURONS = 64\n",
    "DROPOUT_RATE = 0.5\n",
    "ACTIVATION = 'relu'\n",
    "MODEL = 'rn152v2'\n",
    "VERSION = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:49.232937Z",
     "iopub.status.busy": "2024-09-09T11:46:49.232653Z",
     "iopub.status.idle": "2024-09-09T11:46:49.240402Z",
     "shell.execute_reply": "2024-09-09T11:46:49.239628Z",
     "shell.execute_reply.started": "2024-09-09T11:46:49.232906Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['train-cnn'] == 1:\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    if RUN_ON_KAGGLE == 1:\n",
    "        all_image_paths, all_labels = get_image_paths_and_labels(os.path.join(ROOT_IMAGE_DIR,'train'), '/0')\n",
    "    else:\n",
    "        all_image_paths, all_labels = get_image_paths_and_labels(os.path.join(ROOT_IMAGE_DIR,'train'), '\\\\0')\n",
    "\n",
    "    # Frequencies of class 1\n",
    "    n_class_1 = all_labels.count(1)\n",
    "\n",
    "    # Balance the classes by undersampling and oversampling. Both classes will be perfectly balanced.\n",
    "    train_paths_balanced, train_labels_balanced = balance_classes(\n",
    "        all_image_paths, \n",
    "        all_labels, \n",
    "        majority_size=n_class_1 * 10, \n",
    "        minority_size=n_class_1 * 10  \n",
    "    )\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_paths, validation_paths, train_labels, validation_labels = train_test_split(\n",
    "        train_paths_balanced, train_labels_balanced, \n",
    "        test_size=TRAIN_TEST_SPLIT, stratify=train_labels_balanced, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = generate_dataset(train_paths, train_labels, BATCH_SIZE, is_training=True)\n",
    "    validation_dataset = generate_dataset(validation_paths, validation_labels, BATCH_SIZE, is_training=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:49.241741Z",
     "iopub.status.busy": "2024-09-09T11:46:49.241389Z",
     "iopub.status.idle": "2024-09-09T11:46:49.251454Z",
     "shell.execute_reply": "2024-09-09T11:46:49.250549Z",
     "shell.execute_reply.started": "2024-09-09T11:46:49.241689Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(flag=0):\n",
    "    \n",
    "    # Load the ResNet50V2 backbone\n",
    "    #base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(IM_SIZE, IM_SIZE, 3))\n",
    "    base_model = ResNet152V2(weights=None, include_top=False, input_shape=(IM_SIZE, IM_SIZE, 3))\n",
    "    \n",
    "    if flag == 0:\n",
    "        base_model.load_weights(rn152v2_weights)\n",
    "        \n",
    "    # Enable backbone training\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "\n",
    "    # Get the output tensor of the base ResNet50V2 model\n",
    "    base_output = base_model.output\n",
    "\n",
    "    # Apply global Average Pooling\n",
    "    x = GlobalAveragePooling2D()(base_output)    \n",
    "\n",
    "    # Add a fully connected layer with dropout and regularization to prevent overfitting\n",
    "    x = Dense(NEURONS, kernel_initializer=glorot_uniform(seed=42), activation=ACTIVATION, kernel_regularizer=tf.keras.regularizers.l2(REG_RATE))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)    \n",
    "\n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    # Compile de model\n",
    "    optimizer = Adam(learning_rate=LEARN_RATE)\n",
    "    eval_metrics = [\"accuracy\"] #AUC(from_logits=False), SpecificityAtSensitivity(sensitivity=0.8)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=eval_metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Design and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Training a Convolutional Neural Network (CNN) often results in non-deterministic outcomes, meaning the exact same model might not be produced each time, even with identical settings. This is particularly true when using GPUs and applying random data augmentations during training. Althougth seeding mechanisms have been implemented in this notebook to reduce randomness, complete reproducibility is not guaranteed.\n",
    "\n",
    "To address this issue, the notebook will execute the entire training process on the GPU, but for consistent results, the output will be **overridden** by a pre-trained model generated in a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:49.254577Z",
     "iopub.status.busy": "2024-09-09T11:46:49.252523Z",
     "iopub.status.idle": "2024-09-09T11:46:49.265619Z",
     "shell.execute_reply": "2024-09-09T11:46:49.264817Z",
     "shell.execute_reply.started": "2024-09-09T11:46:49.254542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and train the CNN\n",
    "if ENABLE['train-cnn'] == 1:\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        \n",
    "        # Create the model\n",
    "        CNN_MODEL = create_cnn_model(flag=0)\n",
    "\n",
    "        # Define learning rate reduction algorithm\n",
    "        reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5, min_lr=1e-7)\n",
    "\n",
    "        # Define the model checkpoint callback\n",
    "        filepath=f\"{MODEL}_nn{NEURONS}_lr{int(LEARN_RATE * 10000):04}_{ACTIVATION}_batch{BATCH_SIZE}_epoch{EPOCHS}_kaggle_{VERSION}.keras\"\n",
    "        checkpoint_callback = ModelCheckpoint(filepath=filepath, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        history = CNN_MODEL.fit(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,    \n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            validation_data=validation_dataset,    \n",
    "            callbacks=[plot_learning(), checkpoint_callback] #reduce_lr_callback\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:46:49.266908Z",
     "iopub.status.busy": "2024-09-09T11:46:49.266571Z",
     "iopub.status.idle": "2024-09-09T11:46:49.278223Z",
     "shell.execute_reply": "2024-09-09T11:46:49.277382Z",
     "shell.execute_reply.started": "2024-09-09T11:46:49.266875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "if ENABLE['train-cnn'] == 1:\n",
    "    \n",
    "#   CNN_MODEL = load_model(filepath)\n",
    "#   CNN_MODEL.save_weights(f\"{MODEL}_nn{NEURONS}_lr{int(LEARN_RATE * 10000):04}_{ACTIVATION}_batch{BATCH_SIZE}_epoch{EPOCHS}_kaggle_weights_{VERSION}.weights.h5\")\n",
    "    \n",
    "    # Remove the currently generated checkpoint if exists\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:15.374582Z",
     "iopub.status.busy": "2024-09-09T11:47:15.373736Z",
     "iopub.status.idle": "2024-09-09T11:47:22.731360Z",
     "shell.execute_reply": "2024-09-09T11:47:22.730360Z",
     "shell.execute_reply.started": "2024-09-09T11:47:15.374539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a prevously trained model for the sake of reproducibility and consistency in the next sections \n",
    "if ENABLE['train-cnn'] == 1:\n",
    "    if ENABLE['load-cnn-bk'] == 1:\n",
    "        CNN_MODEL = create_cnn_model(flag=1)\n",
    "        CNN_MODEL.load_weights(cnn_weights)\n",
    "else:\n",
    "    CNN_MODEL = create_cnn_model(flag=1)\n",
    "    CNN_MODEL.load_weights(cnn_weights)\n",
    "             \n",
    "if ENABLE['profiling'] == 1:\n",
    "    endt = time.time()\n",
    "    cnn_t = hhmmss(endt - start)\n",
    "    print(f\"\\nCNN training time: {cnn_t}.\")\n",
    "    print(f\"Total time: {hhmmss(endt - start_total)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Machine Learning Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:28.897343Z",
     "iopub.status.busy": "2024-09-09T11:47:28.896899Z",
     "iopub.status.idle": "2024-09-09T11:47:28.910058Z",
     "shell.execute_reply": "2024-09-09T11:47:28.908907Z",
     "shell.execute_reply.started": "2024-09-09T11:47:28.897302Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['profiling'] == 1:\n",
    "    start = time.time()\n",
    "\n",
    "# This function calculates the partial AUC score\n",
    "def partial_auc_score(y_actual, y_scores, tpr_threshold=0.80):\n",
    "    max_fpr = 1 - tpr_threshold\n",
    "\n",
    "    # create numpy arrays\n",
    "    y_actual = np.asarray(y_actual)\n",
    "    y_scores = np.asarray(y_scores)\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_actual, y_scores)\n",
    "\n",
    "    # Find the index where fpr exceeds max_fpr\n",
    "    stop_index = np.searchsorted(fpr, max_fpr, side='right')\n",
    "\n",
    "    if stop_index < len(fpr):\n",
    "        # Interpolate to find the TPR at max_fpr\n",
    "        fpr_interp_points = [fpr[stop_index - 1], fpr[stop_index]]\n",
    "        tpr_interp_points = [tpr[stop_index - 1], tpr[stop_index]]\n",
    "        tpr = np.append(tpr[:stop_index], np.interp(max_fpr, fpr_interp_points, tpr_interp_points))\n",
    "        fpr = np.append(fpr[:stop_index], max_fpr)\n",
    "    else:\n",
    "        tpr = np.append(tpr, 1.0)\n",
    "        fpr = np.append(fpr, max_fpr)\n",
    "\n",
    "    # Calculate partial AUC\n",
    "    partial_auc_value = auc(fpr, tpr)\n",
    "\n",
    "    return partial_auc_value\n",
    "\n",
    "# This fuction calculates the average partial AUC score across all validation folds.\n",
    "def cross_val_partial_auc_score(X, y, model, n_splits):\n",
    "\n",
    "     # Setup cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    pauc_scores = []\n",
    "    cont = 1\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "\n",
    "        print(f'Processing fold {cont} of {n_splits}... ', end='', flush=True)\n",
    "        \n",
    "        # Create the folds\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "        # Train the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "        # Predict on the validation set\n",
    "        preds = model.predict_proba(X_val_fold)[:,1]\n",
    "   \n",
    "        # Calculate partical AUC and store it\n",
    "        pauc = partial_auc_score(y_val_fold, preds)\n",
    "        pauc_scores.append(pauc)\n",
    "\n",
    "        print(f'pAUC: {pauc}', flush=True)\n",
    "        \n",
    "        cont += 1\n",
    "    \n",
    "    # Return the average\n",
    "    return np.mean(pauc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Final Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:29.250164Z",
     "iopub.status.busy": "2024-09-09T11:47:29.249388Z",
     "iopub.status.idle": "2024-09-09T11:47:29.257469Z",
     "shell.execute_reply": "2024-09-09T11:47:29.256617Z",
     "shell.execute_reply.started": "2024-09-09T11:47:29.250119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metadata\n",
    "if ENABLE['train-ml'] == 1:\n",
    "    # For consistency, make sure that the categorical features are labeled as categorical\n",
    "    df_eda_fe_train['anatom_site_general'] = pd.Categorical(df_eda_fe_train['anatom_site_general'])\n",
    "    df_eda_fe_train['tbp_lv_location'] = pd.Categorical(df_eda_fe_train['tbp_lv_location'])\n",
    "    df_eda_fe_train['tbp_lv_location_simple'] = pd.Categorical(df_eda_fe_train['tbp_lv_location_simple'])\n",
    "    df_eda_fe_train['sex'] = pd.Categorical(df_eda_fe_train['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:30.542931Z",
     "iopub.status.busy": "2024-09-09T11:47:30.542127Z",
     "iopub.status.idle": "2024-09-09T11:47:31.279386Z",
     "shell.execute_reply": "2024-09-09T11:47:31.278579Z",
     "shell.execute_reply.started": "2024-09-09T11:47:30.542890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Images\n",
    "\n",
    "# Note: In order to train the final ML pipeline, a previously generated cross-validated CNN predition\n",
    "# vector has been created during the stratifed K-fold cross-validation process to find the best CNN\n",
    "# parameters. Predictions were made out-of-sample using unseen data: for each fold, a CNN model is \n",
    "# trained using 4 folds and predictions are made on the remaining fold. This apprach follows the\n",
    "# principles to ensure the integrity of the final model avoiding data leakage.\n",
    "\n",
    "if ENABLE['train-ml'] == 1:\n",
    "    \n",
    "    # Read the cross-validated predictions\n",
    "    df_cnn_preds = pd.read_csv(crossval_preds)\n",
    "\n",
    "    # Remove column 'target'\n",
    "    df_cnn_preds.drop(columns=['target'], inplace=True)\n",
    "\n",
    "    # Check integrity in the training data\n",
    "    df_merge = pd.merge(df_eda_fe_train, df_cnn_preds, on='isic_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:31.281394Z",
     "iopub.status.busy": "2024-09-09T11:47:31.281059Z",
     "iopub.status.idle": "2024-09-09T11:47:31.636997Z",
     "shell.execute_reply": "2024-09-09T11:47:31.636037Z",
     "shell.execute_reply.started": "2024-09-09T11:47:31.281358Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X: features, y: targets (0: benign - 1: malign)\n",
    "if ENABLE['train-ml'] == 1:\n",
    "    \n",
    "    X = df_merge.drop(['target', 'isic_id'], axis=1)\n",
    "    y = df_merge['target']\n",
    "\n",
    "    # Specify numerical and categorical features\n",
    "    cnn_feature = ['crossval_cnn_preds']\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_features = [col for col in X.columns if col not in numerical_features]\n",
    "    numerical_features.remove('crossval_cnn_preds') # To be removed for the preprocessing pipeline, this feature does not need to be scaled!\n",
    "    \n",
    "    print(f\"Numerical features: {numerical_features} - Length: {len(numerical_features)}\")\n",
    "    print(f\"Categorical features: {categorical_features} - Length: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high amount of features, the K best features can be selected to fed the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:35.855235Z",
     "iopub.status.busy": "2024-09-09T11:47:35.854817Z",
     "iopub.status.idle": "2024-09-09T11:47:39.351148Z",
     "shell.execute_reply": "2024-09-09T11:47:39.350150Z",
     "shell.execute_reply.started": "2024-09-09T11:47:35.855193Z"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE['train-ml'] == 1: \n",
    "    def analyze_kbest(X, y, threshold):\n",
    "    \n",
    "        # Identify numerical and categorical features\n",
    "        numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "        categorical_features = [col for col in X.columns if col not in numerical_features]\n",
    "       \n",
    "        # Use SelectKBest (f_classif) for numerical features\n",
    "        Kbest_numerical = SelectKBest(score_func=f_classif, k='all')\n",
    "        Kbest_numerical.fit(X[numerical_features], y)\n",
    "       \n",
    "        # Extract feature scores and p-values\n",
    "        scores = Kbest_numerical.scores_\n",
    "        pvalues = Kbest_numerical.pvalues_\n",
    "       \n",
    "        # Create a DataFrame to save feature names, scores, and p-values\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'Feature': numerical_features,\n",
    "            'Score': scores,\n",
    "            'P-Value': pvalues\n",
    "        })\n",
    "       \n",
    "        # Sort features by 'Score'\n",
    "        best_feature_scores = feature_scores[feature_scores['P-Value'] < threshold]\n",
    "        sorted_features = best_feature_scores.sort_values(by='Score', ascending=False)\n",
    "       \n",
    "        # Display the sorted features\n",
    "        kbest_num = sorted_features.shape[0]\n",
    "        print(f\"Number of relevant numerical features: {kbest_num}\\n\")\n",
    "        print(sorted_features.set_index('Feature'))\n",
    "       \n",
    "        #best_numerical_features = sorted_features.index.tolist()\n",
    "       \n",
    "        # Plot feature importance\n",
    "        fig = plt.figure(figsize=(20,2))\n",
    "        ax = fig.add_subplot()\n",
    "        sns.barplot(x='Feature', y='Score', data=sorted_features, ax=ax)\n",
    "        plt.xticks(rotation=90, ha='right')\n",
    "        plt.title('Feature Importance - Numerical', fontsize=16)\n",
    "        plt.xlabel('Feature', fontsize=14)\n",
    "        plt.ylabel('Importance', fontsize=14)\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "       \n",
    "        # Use SelectKBest (chi2) for categorical features\n",
    "    \n",
    "        # Build a pipeline\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), categorical_features)\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "       \n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('onehot', preprocessor),\n",
    "            ('kbest', SelectKBest(score_func=chi2, k='all'))\n",
    "        ])\n",
    "       \n",
    "        cat_transformed = pipeline.fit_transform(X, y)\n",
    "        Kbest_categorical = pipeline.named_steps['kbest']\n",
    "       \n",
    "        # Extract feature scores and p-values\n",
    "        scores = Kbest_categorical.scores_\n",
    "        pvalues = Kbest_categorical.pvalues_\n",
    "       \n",
    "        # Extract feature names after one-hot encoding\n",
    "        one_hot_feature_names = pipeline.named_steps['onehot'].transformers_[0][1].get_feature_names_out(categorical_features)\n",
    "       \n",
    "        # Create a DataFrame to hold feature names, scores, and p-values\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'Feature': one_hot_feature_names,\n",
    "            'Score': scores,\n",
    "            'P-Value': pvalues\n",
    "        })\n",
    "       \n",
    "        # Sort features by their scores\n",
    "        best_feature_scores = feature_scores[feature_scores['P-Value'] < threshold]\n",
    "        sorted_features = best_feature_scores.sort_values(by='Score', ascending=False)\n",
    "       \n",
    "        # Display the sorted features\n",
    "        kbest_cat = sorted_features.shape[0]\n",
    "        print(f\"Number of relevant categorical features: {kbest_cat}\\n\")\n",
    "        print(sorted_features.set_index('Feature'))\n",
    "    \n",
    "         # Plot feature importance\n",
    "        fig = plt.figure(figsize=(20,2))\n",
    "        ax = fig.add_subplot()\n",
    "        sns.barplot(x='Feature', y='Score', data=sorted_features, ax=ax)\n",
    "        plt.xticks(rotation=90, ha='right')\n",
    "        plt.title('Feature Importance - Categorical', fontsize=16)\n",
    "        plt.xlabel('Feature', fontsize=14)\n",
    "        plt.ylabel('Importance', fontsize=14)\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "       \n",
    "        #best_categorical_features = sorted_features.index.tolist()\n",
    "       \n",
    "        return kbest_num, kbest_cat\n",
    "    \n",
    "    KBEST_NUM, KBEST_CAT = analyze_kbest(X, y, 0.05) # 5% threshold or P-Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:47:42.845650Z",
     "iopub.status.busy": "2024-09-09T11:47:42.845189Z",
     "iopub.status.idle": "2024-09-09T11:47:42.852425Z",
     "shell.execute_reply": "2024-09-09T11:47:42.851404Z",
     "shell.execute_reply.started": "2024-09-09T11:47:42.845610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disable Kbest: slightly better results are achieved with all the features\n",
    "if ENABLE['train-ml'] == 1: \n",
    "    # Different paths for numerical and categorical features\n",
    "    pipe_num = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('kbest', SelectKBest(score_func=f_classif, k=KBEST_NUM)),   \n",
    "    ])\n",
    "    \n",
    "    pipe_cat = Pipeline([\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)),\n",
    "        ('kbest', SelectKBest(score_func=chi2, k=KBEST_CAT)),    \n",
    "    ])\n",
    "    \n",
    "    preprocessing = ColumnTransformer(transformers=[\n",
    "        ('cnn_pred', 'passthrough', cnn_feature),\n",
    "        ('numerical', pipe_num, numerical_features),\n",
    "        ('categorical',pipe_cat, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model architecture consists of a series of advanced ensemble learning models and a final soft-voting classifier:\n",
    "* Random Forest: bagging\n",
    "* XGBoost: boosting\n",
    "* LightGBM: boosting\n",
    "* CatBoost: boosting\n",
    "* GBM: boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:48:31.605875Z",
     "iopub.status.busy": "2024-09-09T11:48:31.605349Z",
     "iopub.status.idle": "2024-09-09T11:48:31.633876Z",
     "shell.execute_reply": "2024-09-09T11:48:31.632819Z",
     "shell.execute_reply.started": "2024-09-09T11:48:31.605823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# From Optuna:\n",
    "param_rf = {\n",
    "    'random_state':      42,\n",
    "    'bootstrap':         True,\n",
    "    'n_jobs':            -1,\n",
    "    'n_estimators':      423,\n",
    "    'max_features':      'sqrt',\n",
    "    'criterion':         'entropy', \n",
    "    'max_depth':         16,\n",
    "    'min_samples_split': 7,\n",
    "    'min_samples_leaf':  11,    \n",
    "    'class_weight':      'balanced_subsample',\n",
    "}\n",
    "\n",
    "model_rf = ImbPipeline([    \n",
    "    ('preprocessing', preprocessing),\n",
    "    ('undersample', RandomUnderSampler(sampling_strategy={0: 40000}, random_state=42)),\n",
    "    ('oversample', SMOTE(sampling_strategy={1: 4000}, random_state=42)),\n",
    "    ('RF',  BalancedRandomForestClassifier(**param_rf))\n",
    "])\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "# From Optuna:  \n",
    "param_xgb = {\n",
    "    'random_state':       42,    \n",
    "    'n_estimators':       314,\n",
    "    'learning_rate':      0.01598029895340339,\n",
    "    'reg_lambda':         8.74156143244379,\n",
    "    'alpha':              0.5488140237894854,\n",
    "    'max_depth':          12,\n",
    "    'subsample':          0.6055836154106841,\n",
    "    'colsample_bytree':   0.8065993145017623,\n",
    "    'colsample_bylevel':  0.42877642423818046,\n",
    "    'scale_pos_weight':   8.137601492474564,\n",
    "    'eval_metric':        'logloss',\n",
    "    'enable_categorical': True\n",
    "}\n",
    "\n",
    "model_xgb = ImbPipeline([    \n",
    "        ('preprocessing', preprocessing),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy={0: 40000}, random_state=42)),\n",
    "        ('oversample', SMOTE(sampling_strategy={1: 4000}, random_state=42)),\n",
    "        ('RF',  XGBClassifier(**param_xgb))\n",
    "])\n",
    "\n",
    "# LightGBM\n",
    "\n",
    "# From Optuna: \n",
    "param_lgb = {\n",
    "    'random_state':     42,\n",
    "    'objective':        'binary',\n",
    "    'boosting_type':    'gbdt',\n",
    "    'verbosity':        -1, \n",
    "    'n_estimators':     468,\n",
    "    'lambda_l1':        0.06494901485897991,\n",
    "    'lambda_l2':        0.004124742250363338,\n",
    "    'learning_rate':    0.008048319396269177,\n",
    "    'max_depth':        36,\n",
    "    'num_leaves':       44,\n",
    "    'colsample_bytree': 0.4093518762615021,\n",
    "    'colsample_bynode': 0.9949755355913121,\n",
    "    'bagging_fraction': 0.9561339359254485,\n",
    "    'bagging_freq':     1,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'scale_pos_weight': 1.6403781913121338\n",
    "}\n",
    "\n",
    "model_lgb = ImbPipeline([    \n",
    "        ('preprocessing', preprocessing),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy={0: 40000}, random_state=42)),\n",
    "        ('oversample', SMOTE(sampling_strategy={1: 4000}, random_state=42)),\n",
    "        ('LGB',  LGBMClassifier(**param_lgb))\n",
    "])\n",
    "\n",
    "# CatBoost\n",
    "\n",
    "# From Optuna:  \n",
    "param_cb = {\n",
    "    'random_state':     42,\n",
    "    'loss_function':    'Logloss',\n",
    "    'verbose':          False,\n",
    "    'n_estimators':     202,\n",
    "    'max_depth':        14,\n",
    "    'learning_rate':    0.06369630886157396,\n",
    "    'scale_pos_weight': 1.4986002433238226,\n",
    "    'l2_leaf_reg':      9.957793406416098,\n",
    "    'subsample':        0.2993503168795559,\n",
    "    'min_data_in_leaf': 14\n",
    "}\n",
    "\n",
    "model_cb = ImbPipeline([    \n",
    "        ('preprocessing', preprocessing),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy={0: 40000}, random_state=42)),\n",
    "        ('oversample', SMOTE(sampling_strategy={1: 4000}, random_state=42)),\n",
    "        ('CB',  CatBoostClassifier(**param_cb))\n",
    "])\n",
    "\n",
    "# GBM\n",
    "\n",
    "# From Optuna:\n",
    "param_gbm = {\n",
    "    'random_state':      42,\n",
    "    'n_estimators':      409,\n",
    "    'learning_rate':     0.013344792621668246,\n",
    "    'max_depth':         6,\n",
    "    'min_samples_split': 13,\n",
    "    'min_samples_leaf':  12,\n",
    "    'max_features':      'sqrt',\n",
    "    'subsample':         0.5247402463247972\n",
    "}\n",
    "\n",
    "model_gbm = ImbPipeline([    \n",
    "        ('preprocessing', preprocessing),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy={0: 40000}, random_state=42)),\n",
    "        ('oversample', SMOTE(sampling_strategy={1: 4000}, random_state=42)),\n",
    "        ('GBM', GradientBoostingClassifier(**param_gbm))\n",
    "])\n",
    "\n",
    "# Final stacking architecture\n",
    "\n",
    "# Build the soft-voting ensemble archicecture\n",
    "estimators=[\n",
    "        #('RF',  model_rf),\n",
    "        ('XGB', model_xgb),\n",
    "        ('LGB', model_lgb),\n",
    "        #('CB',  model_cb),\n",
    "        ('GBM', model_gbm)\n",
    "]\n",
    "\n",
    "SOFT_VOTING_CLASSIF = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting='soft',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:48:34.634813Z",
     "iopub.status.busy": "2024-09-09T11:48:34.633912Z",
     "iopub.status.idle": "2024-09-09T11:48:34.995758Z",
     "shell.execute_reply": "2024-09-09T11:48:34.994839Z",
     "shell.execute_reply.started": "2024-09-09T11:48:34.634767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the model architecture \n",
    "SOFT_VOTING_CLASSIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Average Partial AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average partial AUC across across the folds\n",
    "#if ENABLE['train-ml'] == 1: \n",
    "#    pauc_sft = cross_val_partial_auc_score(X, y, SOFT_VOTING_CLASSIF, n_splits=5)\n",
    "#    print(f\"CV Partial AUC Score: {pauc_sft}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the whole training dataset\n",
    "if ENABLE['train-ml'] == 1: \n",
    "    print(\"Fitting... \", end='')\n",
    "    SOFT_VOTING_CLASSIF.fit(X, y)\n",
    "    #model_lgb.fit(X, y)\n",
    "    print(\"finished.\")\n",
    "    #print(\"And saving...\")\n",
    "    #dump(SOFT_VOTING_CLASSIF, 'soft_voting_classif_xgb_lgb_gbm_v2.joblib')\n",
    "    #dump(model_lgb, 'model_lgb_withcnn.joblib')\n",
    "    #print(\"finished.\")\n",
    "else:\n",
    "    SOFT_VOTING_CLASSIF = load(ml_model)\n",
    "    \n",
    "if ENABLE['profiling'] == 1:\n",
    "    endt = time.time()\n",
    "    ml_t = hhmmss(endt - start)\n",
    "    print(f\"\\nML model training time: {ml_t}.\")\n",
    "    print(f\"Total time: {hhmmss(endt - start_total)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata: Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['profiling'] == 1:\n",
    "    start = time.time()\n",
    "\n",
    "# Read the test metadata file \n",
    "if ENABLE['inference'] == 1:\n",
    "\n",
    "    # Read the dataset\n",
    "    meta_file = os.path.join(ROOT_DATASET_DIR,\"isic-2024-challenge\",\"test-metadata.csv\")\n",
    "    hdf5_file =os.path.join(ROOT_DATASET_DIR,\"isic-2024-challenge\",\"test-image.hdf5\")\n",
    "    df_raw = pd.read_csv(meta_file)\n",
    "    \n",
    "    print(f\"Number of samples in the test dataframe: {df_raw.shape[0]}\")\n",
    "    print(f\"Number of features in the test dataframe: {df_raw.shape[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Drop redundant columns or columns that are not in the training set\n",
    "    train_columns = df_dropped_train.columns\n",
    "    df_dropped = df_raw[train_columns.intersection(df_raw.columns)]   \n",
    "          \n",
    "    # Replace NaN values\n",
    "    if df_dropped['age_approx'].isnull().any():\n",
    "        IMPUTER_AGE = load(imputer_age_model)\n",
    "        age_approx_reshaped = df_dropped[['age_approx']]\n",
    "        df_dropped['age_approx'] = IMPUTER_AGE.transform(age_approx_reshaped)\n",
    "        \n",
    "    df_dropped['sex'] = df_dropped['sex'].apply(lambda x: 0 if x == 'male' else 1 if x == 'female' else np.NaN)        \n",
    "    if df_dropped['sex'].isnull().any():\n",
    "        IMPUTER_SEX = load(imputer_sex_model)\n",
    "        sex_reshaped = df_dropped[['sex']]\n",
    "        df_dropped['sex'] = IMPUTER_SEX.transform(sex_reshaped)\n",
    "        \n",
    "    # Dropp duplicates\n",
    "    df_dropped.drop_duplicates(inplace=True)  \n",
    "    \n",
    "    # Determine how many samples have been removed\n",
    "    print(f\"Number of samples before removing NaNs and duplicates: {df_raw.shape[0]}\")\n",
    "    print(f\"Number of samples after removing NaNs and duplicates:  {df_dropped.shape[0]}\")\n",
    "    print(f\"Data reduccion in percentage: {np.round(100 * (df_dropped.shape[0] - df_raw.shape[0]) / df_raw.shape[0], 1)}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "if ENABLE['inference'] == 1:\n",
    "    \n",
    "    # Convert: tdb_tile_type -> 0: white, 1: XP\n",
    "    df_dropped['tbp_tile_type'] = df_dropped['tbp_tile_type'].apply(lambda x: 0 if x == '3D: white' else 1)\n",
    "    df_dropped['tbp_tile_type'] = df_dropped['tbp_tile_type'].astype(int)\n",
    "\n",
    "    # Convert: anatom_site_general, tbp_lv_location, tbp_lv_location_simple into categorical, and sex\n",
    "    df_dropped['anatom_site_general'] = pd.Categorical(df_dropped['anatom_site_general'])\n",
    "    df_dropped['tbp_lv_location'] = pd.Categorical(df_dropped['tbp_lv_location'])\n",
    "    df_dropped['tbp_lv_location_simple'] = pd.Categorical(df_dropped['tbp_lv_location_simple'])\n",
    "    df_dropped['sex'] = pd.Categorical(df_dropped['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct skewness and concatenate with the original feature dataframe\n",
    "if ENABLE['inference'] == 1:\n",
    "    \n",
    "    # Sqrt transform \"tbp_lv_eccentricity\"\n",
    "    feature_to_be_sqrtr = ['tbp_lv_eccentricity']\n",
    "    df_sqr_feature = df_dropped[feature_to_be_sqrtr].apply(lambda x : np.square(x))\n",
    "    df_sqr_feature.columns = ['sqr_tbp_lv_eccentricity']\n",
    "    \n",
    "    # Concatenate\n",
    "    df_eda = pd.concat([df_dropped, df_sqr_feature], axis=1)\n",
    "    df_eda = df_eda.reset_index(drop=True)\n",
    "    print(f\"Colums df_dropped_train: {df_dropped.shape[1]}\")\n",
    "    print(f\"Colums df_eda_train: {df_eda.shape[1]}\")\n",
    "    \n",
    "    # Verify that the dataframe has no NaN values\n",
    "    print_NaNs(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering (new features based on metadata) and skewness correction\n",
    "if ENABLE['inference'] == 1:\n",
    "    \n",
    "    # Feature engineering\n",
    "    df_fe = apply_fe(df_eda)\n",
    "    \n",
    "    # And remove tbp_lv_eccentricity\n",
    "    df_eda.drop(feature_to_be_sqrtr, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the partial dataframes\n",
    "if ENABLE['inference'] == 1:\n",
    "    df_eda_fe = pd.concat([df_eda,   # original features = [df_dropped, df_dropped_log_sqr_corr]\n",
    "                           df_fe],   # new features from the orignal ones \n",
    "                           axis=1)\n",
    "    df_eda_fe = df_eda_fe.reset_index(drop=True)\n",
    "    \n",
    "    # Apply imputing if some operations have resulted in NaN values\n",
    "    IMPUTER_NUMERICALS = load(imputer_numerical_model)\n",
    "    IMPUTER_CATEGORICALS = load(imputer_categorical_model)\n",
    "    numericals = df_eda_fe.select_dtypes(include=['number']).columns.tolist()\n",
    "    categoricals = [col for col in df_eda_fe.columns if col not in numericals]\n",
    "    numericals.remove('age_approx') # Already analyzed\n",
    "    categoricals.remove('sex') # Already analyzed\n",
    "    df_eda_fe[numericals] = IMPUTER_NUMERICALS.transform(df_eda_fe[numericals])\n",
    "    df_eda_fe[categoricals] = IMPUTER_CATEGORICALS.transform(df_eda_fe[categoricals])\n",
    "\n",
    "    print(f\"Colums df_eda: {df_eda.shape[1]}\")\n",
    "    print(f\"Colums df_eda + fe (log + sqr + sqrt): {df_eda_fe.shape[1]}\")\n",
    "    \n",
    "    # Verify that the dataframe has no NaN values\n",
    "    print_NaNs(df_eda_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class image dataloader: from https://www.kaggle.com/code/masayukeeeee/how-to-extract-images-from-hdf5-file\n",
    "@dataclass\n",
    "# dataloader\n",
    "class DataLoader():\n",
    "    def __init__(self, meta_file, hdf5_file):\n",
    "        self.meta_file = meta_file\n",
    "        self.hdf5_file = hdf5_file\n",
    "\n",
    "    def load_meta(self):\n",
    "        setattr(self, f\"meta\", pd.read_csv(self.meta_file))\n",
    "            \n",
    "    def load_hdf5(self):        \n",
    "        setattr(self, f\"hdf5\", h5py.File(self.hdf5_file, 'r'))\n",
    "\n",
    "    def __getitem__(self, isic_id):\n",
    "        if self.hdf5.get(isic_id):\n",
    "            return self.hdf5[isic_id][()]\n",
    "        else:\n",
    "            raise ValueError(f\"ISIC_ID {isic_id} is not found in both train and test data.\")                \n",
    "                \n",
    "dl = DataLoader(meta_file, hdf5_file)\n",
    "dl.load_meta()\n",
    "dl.load_hdf5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the images in a test directory and create the dataset input to the CNN\n",
    "if ENABLE['inference'] == 1:\n",
    "    \n",
    "    # Read the dataset\n",
    "    ROOT_IMAGE_DIR =  os.path.join(\"images\")    \n",
    "    ROOT_INFERENCE_DIR = os.path.join(ROOT_IMAGE_DIR,'inference')\n",
    "    ROOT_INFERENCE_LABEL_DIR = os.path.join(ROOT_IMAGE_DIR,'inference','10')\n",
    "\n",
    "    # Create the inference directory if not already present        \n",
    "    os.makedirs(ROOT_INFERENCE_DIR, exist_ok=True)\n",
    "    os.makedirs(ROOT_INFERENCE_LABEL_DIR, exist_ok=True)\n",
    "    \n",
    "    def save_images_from_hdf5(dl, output_dir): #, split='train'): #split='test'):\n",
    "        \n",
    "        # Iterate through the images in the dataset\n",
    "        for i in range(len(dl.meta)):\n",
    "            isic_id = dl.meta.iloc[i].isic_id\n",
    "            img_bytes = dl.__getitem__(isic_id)\n",
    "            img = Image.open(BytesIO(img_bytes))\n",
    "\n",
    "            # Save the image to the specified directory\n",
    "            img.save(os.path.join(output_dir, f\"{isic_id}.jpg\"))\n",
    "    \n",
    "    # Call the function to save images\n",
    "    save_images_from_hdf5(dl, ROOT_INFERENCE_LABEL_DIR)\n",
    "\n",
    "    # Create the test dataset\n",
    "    if RUN_ON_KAGGLE == 1:\n",
    "        test_image_paths, test_labels = get_image_paths_and_labels(ROOT_INFERENCE_DIR, '/10') # Just dummy labels\n",
    "    else:\n",
    "        test_image_paths, test_labels = get_image_paths_and_labels(ROOT_INFERENCE_DIR, '\\\\10') # Just dummy labels\n",
    "\n",
    "    # Check integrity in the test data to match the image list in the image folder and in the dataframe\n",
    "    \n",
    "    # Create a dataframe with image names from test_image_paths\n",
    "    test_image_names = pd.DataFrame({'test_image_paths': test_image_paths, 'test_labels': test_labels})\n",
    "    test_image_names['isic_id'] = test_image_names['test_image_paths'].apply(lambda x: re.search(r'(ISIC_\\d+)', x).group(1))\n",
    "\n",
    "    # Perform an inner merge with the metadata dataframe. The order of the left keys (df_eda_fe) is preserved.\n",
    "    df_merge = pd.merge(df_eda_fe, test_image_names, on='isic_id', how='inner')\n",
    "    \n",
    "    # Extract the paths and (dummy) labels and reordered metadata\n",
    "    test_image_paths = df_merge['test_image_paths'].tolist()\n",
    "    test_labels = df_merge['test_labels'].tolist()\n",
    "\n",
    "    # Generate the dataset, the input of the CNN\n",
    "    test_image_dataset = generate_dataset(test_image_paths, test_labels, BATCH_SIZE, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify consistency in the test feature set\n",
    "if ENABLE['inference'] == 1:\n",
    "\n",
    "    def verify_train_test_feature_match(X_train, X_test):\n",
    "        # Separate numerical and categorical features for both train and test sets\n",
    "        numerical_features_train = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "        categorical_features_train = [col for col in X_train.columns if col not in numerical_features_train]\n",
    "\n",
    "        numerical_features_test = X_test.select_dtypes(include=['number']).columns.tolist()\n",
    "        categorical_features_test = [col for col in X_test.columns if col not in numerical_features_test]\n",
    "\n",
    "        # Check for differences in numerical features\n",
    "        missing_numerical_in_test = set(numerical_features_train) - set(numerical_features_test)\n",
    "        extra_numerical_in_test = set(numerical_features_test) - set(numerical_features_train)\n",
    "\n",
    "        # Check for differences in categorical features\n",
    "        missing_categorical_in_test = set(categorical_features_train) - set(categorical_features_test)\n",
    "        extra_categorical_in_test = set(categorical_features_test) - set(categorical_features_train)\n",
    "\n",
    "        # Check if both numerical and categorical features are identical in both sets\n",
    "        if not missing_numerical_in_test and not extra_numerical_in_test and not missing_categorical_in_test and not extra_categorical_in_test:\n",
    "            print(\"Feature check passed: Both train and test sets have the same features.\")\n",
    "            print(f\"Numerical features: {numerical_features_test} - Length: {len(numerical_features_test)}\")\n",
    "            print(f\"Categorical features: {categorical_features_test} - Length: {len(categorical_features_test)}\")\n",
    "            \n",
    "        else:\n",
    "            # Print out the results\n",
    "            print(\"Feature check failed: Train and test sets have different features:\")\n",
    "\n",
    "            # Printing function\n",
    "            def print_result(description, result_set):\n",
    "                if result_set:\n",
    "                    print(f\"- {description}:\", result_set)\n",
    "                else:\n",
    "                    print(f\"- {description}: None\")\n",
    "\n",
    "            print_result(\"Numerical features in train but not in test\", missing_numerical_in_test)\n",
    "            print_result(\"Numerical features in test but not in train\", extra_numerical_in_test)\n",
    "            print_result(\"Categorical features in train but not in test\", missing_categorical_in_test)\n",
    "            print_result(\"Categorical features in test but not in train\", extra_categorical_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE['inference'] == 1:\n",
    "\n",
    "    # Make predictions using the CNN\n",
    "    cnn_test_preds = CNN_MODEL.predict(test_image_dataset)\n",
    "    df_cnn_test_preds = pd.DataFrame(cnn_test_preds, columns=['crossval_cnn_preds'])\n",
    "\n",
    "    # Make final predictions\n",
    "    X_test = pd.concat([df_eda_fe.drop(['isic_id'], axis=1), df_cnn_test_preds], axis=1)\n",
    "    \n",
    "    verify_train_test_feature_match(X, X_test)\n",
    "        \n",
    "    # Print X_test stats    \n",
    "    print(f\"Shape of the test set: {X_test.shape}\")\n",
    "    \n",
    "    y_test = SOFT_VOTING_CLASSIF.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Create the dataframe\n",
    "    df_submission = pd.DataFrame({'isic_id': df_merge['isic_id'].tolist(), 'target': y_test.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print profiling\n",
    "if ENABLE['profiling'] == 1:\n",
    "    endt = time.time()\n",
    "    inf_t = hhmmss(endt - start)\n",
    "    \n",
    "    # Print profiling results\n",
    "    print(\"\\nProfiling Results:\")\n",
    "    print(f\"{'Phase':<25}{'Time (HH:MM:SS)'}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'EDA':<25}{eda_t}\")\n",
    "    print(f\"{'Feature Engineering':<25}{fe_t}\")\n",
    "    print(f\"{'CNN Training':<25}{cnn_t}\")\n",
    "    print(f\"{'ML Model Training':<25}{ml_t}\")\n",
    "    print(f\"{'Inference':<25}{inf_t}\")\n",
    "    print(f\"{'Total':<25}{hhmmss(endt - start_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output csv file\n",
    "if ENABLE['inference'] == 1:\n",
    "    df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temporal folders containing images\n",
    "if os.path.exists(ROOT_IMAGE_DIR):\n",
    "    # Remove the folder and all its contents\n",
    "    shutil.rmtree(ROOT_IMAGE_DIR)\n",
    "    print(f'Folder \"{ROOT_IMAGE_DIR}\" and all its contents have been deleted.')\n",
    "else:\n",
    "    print(f'Folder \"{ROOT_IMAGE_DIR}\" does not exist.')\n",
    "\n",
    "import os\n",
    "# Remove joblib filesif they exist\n",
    "#joblib_files = [\n",
    "#    'imputer_age.joblib',\n",
    "#    'imputer_sex.joblib',\n",
    "#    'imputer_numericals.joblib',\n",
    "#    'imputer_categoricals.joblib'\n",
    "#]\n",
    "#\n",
    "#for file in joblib_files:\n",
    "#    if os.path.exists(file):\n",
    "#        os.remove(file)\n",
    "#        print(f\"Removed: {file}\")\n",
    "#    else:\n",
    "#        print(f\"File does not exist: {file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "datasetId": 5633051,
     "sourceId": 9302907,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5633784,
     "sourceId": 9303927,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 110862,
     "modelInstanceId": 86621,
     "sourceId": 103342,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 113615,
     "modelInstanceId": 89424,
     "sourceId": 106699,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 113667,
     "modelInstanceId": 89474,
     "sourceId": 106752,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 114800,
     "modelInstanceId": 90578,
     "sourceId": 108143,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 113667,
     "modelInstanceId": 89474,
     "sourceId": 108524,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv_tf_gpu (py310)",
   "language": "python",
   "name": ".venv_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
