{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8df50-5083-46f5-bd51-8a341d9b47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume df is your original DataFrame with 'target' as the label column\n",
    "\n",
    "# Split the dataset into two subsets based on the target label\n",
    "df_class_0 = df[df['target'] == 0]\n",
    "df_class_1 = df[df['target'] == 1]\n",
    "\n",
    "# Perform EDA separately on df_class_0 and df_class_1 here\n",
    "\n",
    "# After EDA, merge the subsets back together, with class 0 data first\n",
    "df_merged = pd.concat([df_class_0, df_class_1])\n",
    "\n",
    "# Extract features and labels\n",
    "X = df_merged.drop(columns=['target'])\n",
    "y = df_merged['target']\n",
    "\n",
    "# Perform a stratified train-test split to maintain the class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Check the class distribution in the train and test sets\n",
    "print(f\"Training set class distribution: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test set class distribution: {y_test.value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b470c-b69f-4058-b865-fe3ae69def1a",
   "metadata": {},
   "source": [
    "## Some links of interests\n",
    "\n",
    "### MFEUsLNet\n",
    "https://www.sciencedirect.com/science/article/pii/S2215098624000181#b0155  \n",
    "\n",
    "From the state of the art:\n",
    "\n",
    "Ghosh et al. [31] proposed **SkinNet-16**, a deep-learning model aimed at distinguishing between benign and malignant skin lesions. Their approach employs advanced neural network architecture to enhance classification accuracy.\n",
    "\n",
    "*Pronab Ghosh, Sami Azam, Ryana Quadir, Asif Karim, F.M. Shamrat, Shohag Kumar Bhowmik, Mirjam Jonkman, Khan Md Hasib, Kawsar Ahmed, SkinNet-16: a deep learning approach to identify benign and malignant skin lesions, Front. Oncol. 12 (2022) 931141.*\n",
    "\n",
    "### EfficientNet V2 B0\n",
    "https://www.kaggle.com/code/matthewjansen/transfer-learning-skin-cancer-classification  \n",
    "\n",
    "### Inception-RestNet-v2\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9759648/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc5872-dfcb-4fa1-904e-dff5f161c227",
   "metadata": {},
   "source": [
    "### Sample Code for using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f95c0e-c87f-427f-a859-c51080bffe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load example dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None])\n",
    "    \n",
    "    # Create the model with these hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    \n",
    "    # We want to maximize accuracy, so return it as a negative value\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object and specify the direction of optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Start the optimization\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)  # n_trials is the number of trials, n_jobs=-1 uses all CPUs\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f'Best trial number: {best_trial.number}')\n",
    "print(f'Best value (accuracy): {best_trial.value}')\n",
    "print(f'Best hyperparameters: {best_trial.params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eedbc2-8827-416c-9a23-6a71a7790d94",
   "metadata": {},
   "source": [
    "# XGBoost imbalance classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11393e-a1dc-47ec-8e22-fa91026ec587",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/xgboost-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcdc35-f82c-4a7f-9dbd-ac67d0b59f67",
   "metadata": {},
   "source": [
    "# Enabling GPUs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96f71a-723d-410d-8e83-da25c14cf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability (before building the model)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462111b-0efe-45eb-a6c3-98ca0a03cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify GPU Device\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs: \", devices)\n",
    "\n",
    "# Set memory growth to avoid allocating all GPU memory upfront\n",
    "for device in devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# Specify a specific GPU device to use (if you have multiple GPUs)\n",
    "with tf.device('/GPU:0'):  # or '/GPU:1', etc.\n",
    "    # Your model code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d816005-50a7-40ac-b0ca-4222edf59876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set GPU Device Visibility\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8dfe8-de4a-4ec4-93be-2ebaa08d66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Limit GPU Memory Usage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs: \", devices)\n",
    "\n",
    "for device in devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    # Or set a specific memory limit if needed\n",
    "    # tf.config.experimental.set_virtual_device_configuration(\n",
    "    #     device,\n",
    "    #     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c85e39-0cc0-465c-b7fd-db50d786c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check Device Placement\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # Define and compile your model here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bebdd-44b8-4409-9f3e-e58de8131d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# Set environment variable to use only the first GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295783fe-36bb-40e4-960c-491a0ebfbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Forcing GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57558dd-b508-4a30-9adf-7ba50f44db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# Set environment variable to use only the first GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Optionally, set memory growth to avoid TensorFlow allocating all GPU memory upfront\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Code below to build the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99323ec-e4fc-45e8-98b1-e10cd9f785be",
   "metadata": {},
   "source": [
    "# Steps to design a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b73f1-d8be-4ec8-aa49-03bff83d0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'feature1': [1.2, 2.3, 3.1, 4.5],\n",
    "    'feature2': [5.1, 3.3, 6.2, 1.9],\n",
    "    'feature3': [7.1, 8.3, 9.4, 10.2],\n",
    "    'label': [0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c4be6-634f-4b64-82b0-89f274982a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create TensorFlow Datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert DataFrame to TensorFlow dataset\n",
    "def df_to_dataset(dataframe, labels, shuffle=True, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe.values, labels.values))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Split into training and validation sets (e.g., 80/20 split)\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, val_X = X[:train_size], X[train_size:]\n",
    "train_y, val_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Generate datasets\n",
    "train_dataset = df_to_dataset(train_X, train_y, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataset = df_to_dataset(val_X, val_y, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9d679-fb72-43a1-a894-019161e43fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Design the Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_X.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f1048-695d-45ef-821d-ae8cec9a4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train the Model\n",
    "# Checkpoint callbacks\n",
    "best_checkpoint_path = f\"../models/tabular_nn_best.keras\"\n",
    "best_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=best_checkpoint_path, save_best_only=True)\n",
    "\n",
    "final_checkpoint_path = f\"../models/tabular_nn_final.keras\"\n",
    "final_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=final_checkpoint_path)\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[best_checkpoint_callback, final_checkpoint_callback, reduce_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffe5d6-d9b9-4a4f-ae49-9d681e857d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loading and Continuing Training\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(best_checkpoint_path)\n",
    "\n",
    "# Continue training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,  # Continue for more epochs\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[best_checkpoint_callback, final_checkpoint_callback, reduce_lr_callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_common",
   "language": "python",
   "name": ".venv_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
